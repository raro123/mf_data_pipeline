# Mutual Fund Data Pipeline - Execution Order

## Current Active Scripts (Post-DuckDB Migration)

### ðŸ“‚ Active Scripts Directory: `/scripts/`
1. **`01_fetch_historical_nav.py`** - Historical data fetcher
2. **`02_clean_historical_nav_duckdb.py`** - DuckDB-based historical processor  
3. **`03_fetch_daily_nav.py`** - Daily NAV fetcher with gap-filling
4. **`04_create_combined_table.py`** - DuckDB-based data combiner
5. **`05_extract_scheme_metadata.py`** - Metadata extractor
6. **`06_clean_scheme_metadata.py`** - Enhanced metadata processor
7. **`07_create_analytical_nav_daily_duckdb.py`** - DuckDB analytical view creator
8. **`ingest_zerodha_mf.py`** - Optional Zerodha integration

### ðŸ“‚ Archived Scripts: `/scripts/archive/`
- Legacy memory-based processing scripts
- Experimental and duplicate implementations

---

## ðŸš€ Execution Workflows

### Initial Setup (One-time only)
```bash
# 1. Download historical data (if not already done)
python scripts/01_fetch_historical_nav.py

# 2. Process ALL historical CSV files (2006-2025) using DuckDB
python scripts/02_clean_historical_nav_duckdb.py
```

### Daily/Weekly Pipeline
```bash
# 1. Get latest daily NAV data
python scripts/03_fetch_daily_nav.py

# 2. Get fresh metadata (weekly recommended)
python scripts/05_extract_scheme_metadata.py

# 3. Process metadata with enhanced classifications
python scripts/06_clean_scheme_metadata.py

# 4. Create final analytical dataset
python scripts/07_create_analytical_nav_daily_duckdb.py
```

### Full Refresh Pipeline
```bash
# If you need to rebuild everything from scratch:
python scripts/02_clean_historical_nav_duckdb.py  # Historical data
python scripts/03_fetch_daily_nav.py              # Daily updates  
python scripts/05_extract_scheme_metadata.py      # Fresh metadata
python scripts/06_clean_scheme_metadata.py        # Enhanced processing
python scripts/07_create_analytical_nav_daily_duckdb.py  # Final analytical view
```

---

## ðŸ“Š Current Data Status

### âœ… Completed Components
- **Historical Data**: 25.6M records (2006-2025) processed via DuckDB
- **Daily Updates**: Gap-filled through September 2025
- **Enhanced Metadata**: 16K schemes with Direct/Regular and Growth/Dividend classification
- **Analytical Dataset**: Ready for analysis with all enhancements

### ðŸ”§ Technical Improvements
- **Memory Efficient**: DuckDB handles large datasets without memory issues
- **Performance**: ~15 seconds to process 89 CSV files (vs hours with pandas)
- **Scalable**: Can handle 25M+ records without performance degradation
- **Robust**: Professional logging and error handling throughout

---

## ðŸŽ¯ Key Features

### Enhanced Scheme Classification
- **Direct vs Regular Plans**: Automatic detection from scheme names
- **Growth vs Dividend/IDCW**: Comprehensive pattern matching
- **Category Levels**: Level 1 (5 main types) + Level 2 (49 sub-categories)

### Data Quality
- **Validation**: NAV range validation, null checking
- **Deduplication**: Automatic removal of duplicate records
- **Gap Filling**: Intelligent daily data fetching with missing date detection

### Performance Optimizations
- **DuckDB Integration**: Memory-efficient processing for large datasets
- **Categorical Types**: Optimized storage for repeated string values
- **Parquet Format**: Compressed columnar storage with Snappy compression

---

## ðŸ“‹ Next Steps

1. **Test the complete pipeline** with the DuckDB-based analytical script
2. **Set up scheduling** for daily/weekly execution
3. **Implement monitoring** and alerting for pipeline health
4. **Consider Prefect integration** for workflow orchestration (future enhancement)

The pipeline is now optimized, organized, and ready for production use! ðŸŽ‰